{"cells":[{"metadata":{"_uuid":"5687da5f-cbf2-4da5-9948-485a0c7853c6","_cell_guid":"5ca7927c-6eaa-4eac-8d8a-8a7639747b01","trusted":true},"cell_type":"code","source":"\nimport tensorflow as tf\n\n\nimport tensorflow_datasets as tfds\nimdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n\n\nimport numpy as np\n\ntrain_data, test_data = imdb['train'], imdb['test']\n\ntraining_sentences = []\ntraining_labels = []\n\ntesting_sentences = []\ntesting_labels = []\n\n\nfor s,l in train_data:\n  training_sentences.append(str(s.numpy()))\n  training_labels.append(l.numpy())\n  \nfor s,l in test_data:\n  testing_sentences.append(str(s.numpy()))\n  testing_labels.append(l.numpy())\n  \ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)\n\n\n\ntraining_sentences[0]\n\n\ntraining_labels[0]\n\n\nvocab_size = 10000\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\noov_tok = \"<OOV>\"\n\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(training_sentences)\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)\n\n\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\n\nnum_epochs = 10\nhistory = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))\n\n\nscore = model.evaluate(testing_padded, testing_labels_final)\n\n\n\nprint('Test accuracy:', score[1])","execution_count":13,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 120, 16)           160000    \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1920)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 6)                 11526     \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 7         \n=================================================================\nTotal params: 171,533\nTrainable params: 171,533\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 25000 samples, validate on 25000 samples\nEpoch 1/10\n25000/25000 [==============================] - 7s 284us/sample - loss: 0.4972 - accuracy: 0.7448 - val_loss: 0.3474 - val_accuracy: 0.8472\nEpoch 2/10\n25000/25000 [==============================] - 6s 258us/sample - loss: 0.2415 - accuracy: 0.9074 - val_loss: 0.3700 - val_accuracy: 0.8402\nEpoch 3/10\n25000/25000 [==============================] - 6s 246us/sample - loss: 0.0929 - accuracy: 0.9755 - val_loss: 0.4553 - val_accuracy: 0.8235\nEpoch 4/10\n25000/25000 [==============================] - 6s 249us/sample - loss: 0.0245 - accuracy: 0.9967 - val_loss: 0.5405 - val_accuracy: 0.8231\nEpoch 5/10\n25000/25000 [==============================] - 6s 245us/sample - loss: 0.0066 - accuracy: 0.9994 - val_loss: 0.5952 - val_accuracy: 0.8259\nEpoch 6/10\n25000/25000 [==============================] - 7s 276us/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6491 - val_accuracy: 0.8258\nEpoch 7/10\n25000/25000 [==============================] - 7s 263us/sample - loss: 8.7637e-04 - accuracy: 1.0000 - val_loss: 0.6916 - val_accuracy: 0.8277\nEpoch 8/10\n25000/25000 [==============================] - 6s 255us/sample - loss: 4.9061e-04 - accuracy: 1.0000 - val_loss: 0.7317 - val_accuracy: 0.8282\nEpoch 9/10\n25000/25000 [==============================] - 7s 268us/sample - loss: 2.7768e-04 - accuracy: 1.0000 - val_loss: 0.7689 - val_accuracy: 0.8284\nEpoch 10/10\n25000/25000 [==============================] - 6s 259us/sample - loss: 1.6386e-04 - accuracy: 1.0000 - val_loss: 0.8056 - val_accuracy: 0.8286\n25000/25000 [==============================] - 2s 75us/sample - loss: 0.8056 - accuracy: 0.8286\nTest accuracy: 0.8286\n","name":"stdout"}]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}